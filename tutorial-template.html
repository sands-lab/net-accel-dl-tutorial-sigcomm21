<!-- MARCO -->
<html>
<body>
<!-- /MARCO -->
<div class="content-primary">

<h1 id="call-for-tutorials">ACM SIGCOMM 2021 TUTORIAL: Network-Accelerated Distributed Deep Learning<h1>

<h2 id="-tutorial-program-subject-to-changes"><i class="fa fa-calendar" aria-hidden="true"></i>Tutorial Program (subject to changes)</h2>

<p>The tutorial has an associated Slack channel for discussions. Click on the link below to visit it. If you're asked to sign in, use the workspace name &quot;sigcomm.slack.com&quot; to sign up or sign in.</p>

<a class="button" data-role="button" href="TODO">Go to tutorial Slack channel></a>
<br/>

<div id="tutorial-mptp-program" class="tutorial-mptp-program">
    <ul class="program" data-role="listview" data-filter="true" data-inset="true" data-theme="d" data-dividertheme="a" placeholder="Filter program...">
    <li class="prog-header prog-no-filter ui-bar-a prog-item prog-monday">
    <h3>*** DATE ***</h3>
    </li>

    <li class="ui-li ui-bar-a prog-item ui-li-divider prog-monday   " data-role="list-divider">
    <table>
      <tr>
        <td style="width:100%;text-align:left;">
          <h3 style="font-weight: 700">
            1:15 pm - 3:00 pm
            Session I
          </h3>
        </td>
      </tr>
    </table>
    </li>

    <li class="hidden">Session I</li>

    <li data-icon="false" class="prog-item prog-monday">
        <table>
           <tr>
               <td style="font-size:80%; padding-right:10px; max-width:20%;">1:15 pm - 3:00 pm (1h 45m)</td>

               <td style="width:100%;text-align:left;">
                  <p class="paper-header">
                    Distributed training overview; When the network is the bottleneck; In-network aggregation: SwitchML.
                  </p>
                  <!-- /* the spaces after various "%s" below are important for correct list filtering! */ -->
                  <p>Hands-on: Running a distributed DL job; distributed training with SwitchML; the SwitchML API.</p>
               </td>
           </tr>
        </table>
    </li>

    <li class="prog-break prog-item ui-li-divider prog-monday   " data-role="list-divider">
    <h3 style="font-weight: 500;">
      <strong>3:00 pm - 3:30 pm
      Coffee/tea Break</strong>
    </h3>
    </li>

    <li class="hidden">Coffee/tea Break</li>

    <li class="ui-li ui-bar-a prog-item ui-li-divider prog-monday" data-role="list-divider">
    <table>
      <tr>
        <td style="width:100%;text-align:left;">
          <h3 style="font-weight: 700">
            3:30 pm - 5:05 pm
            Session II
          </h3>
        </td>
      </tr>
    </table>
    </li>

    <li class="hidden">Session II</li>

    <li data-icon="false" class="prog-item prog-monday">
        <table>
           <tr>
               <td style="font-size:80%; padding-right:10px; max-width:20%;">3:30 pm - 5:05 pm (1h 35m)</td>

               <td style="width:100%;text-align:left;">
                  <p class="paper-header">
                    Sparse collective communication: OmniReduce; Gradient compression: GRACE.
                  </p>
                  <!-- /* the spaces after various "%s" below are important for correct list filtering! */ -->
                  <p>Hands-on: Distributed training with OmniReduce; Implement and run a gradient compression algorithm with GRACE.</p>
               </td>
           </tr>
        </table>
    </li>
  </ul>
</div>

<h2 id="call-for-participation">Call For Participation</h2>

<p>Distributed Deep Learning (DL) is an important workload that is becoming communication bound as the scale and size of DL models increases. This tutorial will present a range of techniques that are effective at mitigating the network communication bottleneck and accelerate the performance of distributed training.</p>

<!-- <h2 id="-important-dates"><i class="fa fa-calendar"></i> Important Dates</h2> -->

<!-- <ul data-role="listview" data-inset="true" data-theme="a" data-content-theme="a" class="datetbl"> -->
<!--     <li data-icon="false"> -->
<!--       <div class="ui-grid-a"> -->
<!--         <div class="ui-block-a"><h2>August 10, 2020</h2></div> -->
<!--         <div class="ui-block-b"><p>Tutorial</p></div> -->
<!--       </div> -->
<!--     </li> -->
<!-- </ul> -->

<h2 id="outline">Outline</h2>

<p>Training Deep Neural Network (DNN) models in parallel on a distributed machine cluster is an emergent important workload and increasingly, communication bound.
To be clear, it remains computationally intensive. But the last seven years have brought a 62Ã— improvement in compute performance, thanks to GPUs and other hardware accelerators.
Cloud network deployments have found this pace hard to match, skewing the ratio of computation to communication towards the latter. Meanwhile, in pursuit of ever higher accuracy, data scientists continue to enlarge model sizes and complexity as the underlying compute and memory capabilities allow them to.</p>

<p>The MLSys community has recently been tackling the communication challenges in distributed DNN training with various approaches ranging from efficient parameter servers [1,2] or scalable collective communication [3,4] to in-network aggregation [5] and gradient compression techniques [6,7]. The overarching goal of these works has been to alleviate the communication bottlenecks by reducing the time that workers spend on overall network communication to exchange the local gradients.</p>

<p>In this tutorial, we will present some of the state-of-the-art approaches, primarily focusing on our own work in the area.
We hope the tutorial will familiarize the attendants with this timely area and stimulate discussions and new ideas.</p>

<p>A rough outline follows:</p>

<ul>
    <li><p><strong>Session I </strong>In the first session, we will will provide an introduction on scaling distributed machine learning from a networking-centric perspective. After reviewing basic concepts in distributed DNN training, we will step through different solutions of how to accelerate network communication. We will start with in-network aggregation, describing <a href="https://sands.kaust.edu.sa/project/switchml/">SwitchML</a> as an example of co-design of programmable switch-based processing and end-host protocols. </p></li>
    <li><p><strong>Session II </strong>In the second session, we will look into the properties of the traffic and exploit the sparsity of gradient values. We will describe <a href="https://sands.kaust.edu.sa/project/omnireduce/">OmniReduce</a>, which evolves the concept of in-network aggregation and focuses on efficient collective operations for sparse data. We will close with lossy gradient compression techniques and the <a href="https://sands.kaust.edu.sa/project/grace/">GRACE</a> framework for implementing them.</p></li>
</ul>

<h2 id="audience-expectations-and-prerequisites">Audience Expectations and Prerequisites</h2>

<p>Anyone with basic understanding of networking and machine learning can participate in this tutorial. In order to benefit from the hands-on, they are asked to have access to a computer with Docker installed. More specific instructions TBA.</p>

<h2 id="organizers">Organizers</h2>

<!-- <div  class="presenters"> -->
<ul class="program" data-role="listview" data-inset="true" data-theme="d" data-dividertheme="a">

      <li data-icon="false" class="prog-item">
        <div data-role="collapsible" class="paper-navgroup" data-collapsed="true" data-iconpos="right" data-collapsed-icon="carat-d" data-expanded-icon="carat-u">
          <h4>
            <div>
              <p class="paper-header">
                <div class="ui-grid-a">
                  <div class="ui-block-a"><p>Marco Canini</p></div>
                  <div class="ui-block-b"><p>KAUST</p></div>
                </div>
              </p>
            </div>
            <div class="ui-li-aside">
            </div>
          </h4>
          <ul data-role="listview" data-inset="false">
            <li data-icon="false" style="padding:1px;">
              <!--<p>-->
              <table style="border-spacing: 10px; border-collapse: separate;">
                <tr>



                  <td style="width:90%;">
                  <p><span style="font-weight: 700;">Bio: </span>
                    <p>Marco does not know what the next big thing will be. But he's sure that our next-gen computing and networking infrastructure must be a viable platform for it and avoid stifling innovation. Marco's research spans a number of areas in computer systems, including distributed systems, large-scale/cloud computing and computer networking with emphasis on programmable networks. His current focus is on designing better systems support for AI/ML and providing practical implementations deployable in the real-world.<br />
                    Marco is an associate professor in Computer Science at KAUST. Marco obtained his Ph.D. in computer science and engineering from the University of Genoa in 2009 after spending the last year as a visiting student at the University of Cambridge. He was a postdoctoral researcher at EPFL and a senior research scientist at Deutsche Telekom Innovation Labs & TU Berlin. Before joining KAUST, he was an assistant professor at UCLouvain. He also held positions at Intel, Microsoft and Google.</p>
                  </p><br />
                  </td>
                </tr>
              </table>
            </li>
          </ul>
        </div>
      </li>

      <li data-icon="false" class="prog-item">
        <div data-role="collapsible" class="paper-navgroup" data-collapsed="true" data-iconpos="right" data-collapsed-icon="carat-d" data-expanded-icon="carat-u">
          <h4>
            <div>
              <p class="paper-header">
                <div class="ui-grid-a">
                  <div class="ui-block-a"><p>Jiawei Fei</p></div>
                  <div class="ui-block-b"><p>NUDT and KAUST</p></div>
                </div>
              </p>
            </div>
            <div class="ui-li-aside">
            </div>
          </h4>
          <ul data-role="listview" data-inset="false">
            <li data-icon="false" style="padding:1px;">
              <!--<p>-->
              <table style="border-spacing: 10px; border-collapse: separate;">
                <tr>



                  <td style="width:90%;">
                  <p><span style="font-weight: 700;">Bio: </span>
                    <p>Jiawei works on accelerating distributed machine learning systems by maximizing effective network bandwidth. His interests are mainly in data center network and programmable switches. Jiawei is a Ph.D student in Computer Science, at NUDT and currently is a visiting student at KAUST.</p>
                  </p><br />
                  </td>
                </tr>
              </table>
            </li>
          </ul>
        </div>
      </li>

      <li data-icon="false" class="prog-item">
        <div data-role="collapsible" class="paper-navgroup" data-collapsed="true" data-iconpos="right" data-collapsed-icon="carat-d" data-expanded-icon="carat-u">
          <h4>
            <div>
              <p class="paper-header">
                <div class="ui-grid-a">
                  <div class="ui-block-a"><p>Chen-Yu Ho</p></div>
                  <div class="ui-block-b"><p>KAUST</p></div>
                </div>
              </p>
            </div>
            <div class="ui-li-aside">
            </div>
          </h4>
          <ul data-role="listview" data-inset="false">
            <li data-icon="false" style="padding:1px;">
              <!--<p>-->
              <table style="border-spacing: 10px; border-collapse: separate;">
                <tr>



                  <td style="width:90%;">
                  <p><span style="font-weight: 700;">Bio: </span>
                    <p>Chen-Yu works on developing efficient distributed machine learning systems, focusing on alleviating the network bandwidth bottleneck. His effort spans gradient compression techniques and system designs that exploit characteristics of machine learning workloads. Before joining KAUST as a Ph.D. student, Chen-Yu worked on digitalizing handwriting and ancient Chinese calligraphy arts at Academia Sinica, Taiwan.</p>
                  </p><br />
                  </td>
                </tr>
              </table>
            </li>
          </ul>
        </div>
      </li>

      <li data-icon="false" class="prog-item">
        <div data-role="collapsible" class="paper-navgroup" data-collapsed="true" data-iconpos="right" data-collapsed-icon="carat-d" data-expanded-icon="carat-u">
          <h4>
            <div>
              <p class="paper-header">
                <div class="ui-grid-a">
                  <div class="ui-block-a"><p>Jacob Nelson</p></div>
                  <div class="ui-block-b"><p>Microsoft Research</p></div>
                </div>
              </p>
            </div>
            <div class="ui-li-aside">
            </div>
          </h4>
          <ul data-role="listview" data-inset="false">
            <li data-icon="false" style="padding:1px;">
              <!--<p>-->
              <table style="border-spacing: 10px; border-collapse: separate;">
                <tr>



                  <td style="width:90%;">
                  <p><span style="font-weight: 700;">Bio: </span>
                  <p>Jacob's research explores how emerging datacenter
                  hardware can be used to build faster and more
                  efficient distributed systems. He joined the Systems
                  Research Group at Microsoft Research's Redmond Lab
                  in 2016 after completing his Ph. D. at the Allen
                  School of Computer Science and Engineering at the
                  University of Washington.
                    </p>
                  </p><br />
                  </td>
                </tr>
              </table>
            </li>
          </ul>
        </div>
      </li>

      <li data-icon="false" class="prog-item">
        <div data-role="collapsible" class="paper-navgroup" data-collapsed="true" data-iconpos="right" data-collapsed-icon="carat-d" data-expanded-icon="carat-u">
          <h4>
            <div>
              <p class="paper-header">
                <div class="ui-grid-a">
                  <div class="ui-block-a"><p>Amedeo Sapio</p></div>
                  <div class="ui-block-b"><p>Intel</p></div>
                </div>
              </p>
            </div>
            <div class="ui-li-aside">
            </div>
          </h4>
          <ul data-role="listview" data-inset="false">
            <li data-icon="false" style="padding:1px;">
              <!--<p>-->
              <table style="border-spacing: 10px; border-collapse: separate;">
                <tr>



                  <td style="width:90%;">
                  <p><span style="font-weight: 700;">Bio: </span>
                    <p> Amedeo is leading the effort to support in-network computation in programmable switches within Intel. His main research interests include high-speed packet processing, dataplane programming and innovative network services. Before joining Intel, he was a Software Engineer with the Cisco Data Center Switching Group, a PostDoctoral Researcher at KAUST and a Visiting Researcher at Narus. Amedeo obtained his Ph.D. in computer engineering from Politecnico di Torino. </p>
                  </p><br />
                  </td>
                </tr>
              </table>
            </li>
          </ul>
        </div>
      </li>

  </ul>
<p><!-- </div> --></p>

<h3 id="references">References</h3>

<p> [1] L. Luo, J. Nelson, L. Ceze, A. Phanishayee, and A. Krishnamurthy.  PHub: Rack-Scale Parameter Server for Distributed Deep Neural Network Training. In <i>SoCC</i>, 2018.</p>

<p> [2] Y. Jiang, Y. Zhu, C. Lan, B. Yi, Y. Cui, and C. Guo.  A Unified Architecturefor Accelerating Distributed DNN Training in Heterogeneous GPU/CPU Clusters. In <i>OSDI</i>, 2020.</p>

<p> [3] G. Wang, S. Venkataraman, A. Phanishayee, J. Thelin, N. Devanur, and I. Stoica.  Blink: Fast and Generic Collectives for Distributed ML.  In <i>MLSys</i>, 2020.</p>

<p> [4] J. Fei, C.-Y. Ho, A. N. Sahu, M. Canini, and A. Sapio.  Efficient SparseCollective Communication and its application to Accelerate DistributedDeep Learning. In <i>SIGCOMM</i>, 2021.</p>

<p> [5] A. Sapio, M. Canini, C.-Y. Ho, J. Nelson, P. Kalnis, C. Kim, A. Krishnamurthy, M. Moshref, D. R. K. Ports, and P. Richtarik.  Scaling Distributed Machine Learning with In-Network Aggregation. In <i>NSDI</i>, 2021.</p>

<p> [6] H. Xu, C.-Y. Ho, A. M. Abdelmoniem, A. Dutta, E. H. Bergou, K. Karatsenidis, M. Canini, and P. Kalnis.  GRACE: A Compressed Communication Framework for Distributed Machine Learning. In <i>ICDCS</i>, 2021.</p>

<p> [7] A. M. Abdelmoniem, A. Elzanaty, M.-S. Alouini, and M. Canini. An Efficient Statistical-based Gradient Compression Technique for Distributed Training Systems. In <i>MLSys</i>, 2021.</p>

</div><!-- content-primary -->
<!-- MARCO -->
</body>
</html>
<!-- /MARCO -->
